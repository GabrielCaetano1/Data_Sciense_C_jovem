{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ed08e3",
   "metadata": {},
   "source": [
    "\n",
    "# Processamento Textual: Teoria e Prática com Word2Vec, Clusterização e LSTMs\n",
    "\n",
    "Este notebook foi desenvolvido para uma aula completa com explicações teóricas e práticas. Ele aborda processamento textual \n",
    "em profundidade e inclui análises detalhadas de cada etapa.\n",
    "\n",
    "### Estrutura do Notebook\n",
    "1. **Introdução ao Processamento Textual**:\n",
    "   - Contexto geral sobre processamento de linguagem natural (NLP).\n",
    "   - Objetivos do notebook.\n",
    "2. **Embeddings Word2Vec**:\n",
    "   - O que são embeddings e como Word2Vec funciona.\n",
    "   - Geração de embeddings a partir de um corpus sintético.\n",
    "3. **Pré-processamento de Texto**:\n",
    "   - Técnicas de limpeza de texto, stemming e lematização.\n",
    "   - Explicação detalhada do dataset `movie_reviews` do NLTK.\n",
    "4. **Clusterização de Palavras**:\n",
    "   - Explicação de K-Means e como agrupar embeddings.\n",
    "   - Visualização dos clusters e da similaridade entre palavras.\n",
    "5. **Redes LSTM**:\n",
    "   - O que são redes LSTM e como são usadas em NLP.\n",
    "   - Treinamento e análise detalhada de uma LSTM para modelagem de sequência.\n",
    "\n",
    "Cada seção inclui uma explicação teórica antes da execução do código para facilitar o entendimento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a3381d",
   "metadata": {},
   "source": [
    "\n",
    "## Embeddings Word2Vec\n",
    "\n",
    "Os embeddings são representações vetoriais das palavras em um espaço contínuo, permitindo que palavras semanticamente semelhantes fiquem próximas nesse espaço. \n",
    "O Word2Vec é um dos modelos mais populares para criar embeddings e utiliza dois métodos principais:\n",
    "\n",
    "- **Skip-Gram**: Prediz o contexto de uma palavra a partir da palavra alvo.\n",
    "- **CBOW (Continuous Bag of Words)**: Prediz a palavra alvo a partir de seu contexto.\n",
    "\n",
    "Nesta seção, vamos criar embeddings utilizando um corpus sintético simples. O objetivo é demonstrar como treinar um modelo Word2Vec e analisar as representações geradas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20441616",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Corpus sintético simples\n",
    "corpus = [\n",
    "    \"rei homem mulher rainha\",\n",
    "    \"rei rainha trono castelo\",\n",
    "    \"homem mulher menino menina\",\n",
    "    \"rei castelo cavaleiro rainha\",\n",
    "]\n",
    "\n",
    "# Tokenização\n",
    "tokenized_corpus = [word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, workers=4)\n",
    "print(\"Modelo Word2Vec treinado!\")\n",
    "\n",
    "# Análise de embeddings\n",
    "print(\"Vetor para 'rei':\", model.wv[\"rei\"][:5])\n",
    "print(\"Palavras semelhantes a 'rei':\", model.wv.most_similar(\"rei\", topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525ef86c",
   "metadata": {},
   "source": [
    "\n",
    "## Pré-processamento de Texto\n",
    "\n",
    "O pré-processamento é uma etapa essencial em NLP. Ele prepara os dados textuais para análise e modelos de aprendizado. \n",
    "As etapas comuns incluem:\n",
    "\n",
    "1. **Tokenização**: Divisão do texto em palavras ou frases.\n",
    "2. **Remoção de Stopwords**: Palavras comuns como \"o\", \"de\", \"a\" que não contribuem para a análise.\n",
    "3. **Stemming**: Redução das palavras às suas raízes (ex.: \"jogando\" → \"jog\").\n",
    "4. **Lematização**: Redução das palavras às suas formas base (ex.: \"jogando\" → \"jogar\").\n",
    "\n",
    "### Dataset: `movie_reviews` (NLTK)\n",
    "- Este dataset contém resenhas de filmes categorizadas como positivas ou negativas.\n",
    "- É frequentemente usado para análise de sentimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed822320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"movie_reviews\")\n",
    "\n",
    "# Carregar o dataset\n",
    "texts = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()[:3]]\n",
    "\n",
    "# Tokenização e remoção de stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_texts = []\n",
    "for text in texts:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    filtered_texts.append(tokens)\n",
    "\n",
    "# Stemming e lematização\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmed = [[stemmer.stem(word) for word in tokens] for tokens in filtered_texts]\n",
    "lemmatized = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_texts]\n",
    "\n",
    "print(\"Exemplo de texto processado (stemming):\", stemmed[0][:10])\n",
    "print(\"Exemplo de texto processado (lematização):\", lemmatized[0][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0dd303",
   "metadata": {},
   "source": [
    "\n",
    "## Clusterização de Palavras\n",
    "\n",
    "A clusterização é uma técnica de aprendizado não supervisionado para agrupar dados similares. \n",
    "Usaremos o algoritmo **K-Means** para agrupar palavras baseadas em seus embeddings.\n",
    "\n",
    "Além disso, visualizaremos:\n",
    "- Um gráfico de dispersão dos clusters.\n",
    "- Uma matriz de similaridade entre palavras usando um heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e58898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Clusterização com K-Means\n",
    "words = list(model.wv.index_to_key)\n",
    "vectors = np.array([model.wv[word] for word in words])\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "labels = kmeans.fit_predict(vectors)\n",
    "\n",
    "# Visualização dos clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(vectors[i, 0], vectors[i, 1], c=f\"C{labels[i]}\")\n",
    "    plt.text(vectors[i, 0] + 0.02, vectors[i, 1] + 0.02, word, fontsize=9)\n",
    "plt.title(\"Clusters de Palavras\")\n",
    "plt.show()\n",
    "\n",
    "# Matriz de similaridade\n",
    "similarity_matrix = np.dot(vectors, vectors.T)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, xticklabels=words, yticklabels=words, cmap=\"coolwarm\")\n",
    "plt.title(\"Matriz de Similaridade entre Palavras\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d0168",
   "metadata": {},
   "source": [
    "#### O que o gráfico representa?\n",
    "Eixos (X e Y):\n",
    "\n",
    "Estes eixos correspondem a uma projeção bidimensional dos embeddings, que originalmente possuem 50 dimensões (como definido no treinamento do Word2Vec).\n",
    "A redução para 2 dimensões foi feita para possibilitar a visualização gráfica, provavelmente utilizando um método como PCA ou TSNE.\n",
    "Pontos:\n",
    "\n",
    "Cada ponto no gráfico representa uma palavra do corpus.\n",
    "As posições dos pontos refletem a similaridade semântica entre as palavras no espaço vetorial. Palavras próximas têm significados ou contextos semelhantes.\n",
    "Cores:\n",
    "\n",
    "As cores indicam os clusters formados pelo algoritmo K-Means.\n",
    "As palavras dentro do mesmo cluster compartilham características similares no espaço vetorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912aed0f",
   "metadata": {},
   "source": [
    "#### Análise detalhada\n",
    "###### Cluster Azul:\n",
    "- Contém palavras como \"rei\", \"castelo\", e \"trono\".\n",
    "- Essas palavras são semanticamente relacionadas, pois todas estão associadas ao contexto de realeza e ambientes monárquicos.\n",
    "- Isso mostra que o modelo Word2Vec capturou corretamente as relações semânticas do corpus.\n",
    "###### Cluster Laranja:\n",
    "- Inclui palavras como \"homem\", \"mulher\", \"menino\", e \"menina\".\n",
    "- Essas palavras estão relacionadas a gêneros e faixas etárias, formando um agrupamento distinto dos termos relacionados à realeza.\n",
    "\n",
    "###### Posições específicas:\n",
    "- \"Rei\" e \"trono\": Estão muito próximos, indicando que o modelo aprendeu uma relação semântica forte entre eles (o que faz sentido, pois \"rei\" geralmente está associado a um trono no corpus).\n",
    "- \"Mulher\": Está mais distante de \"rei\" e \"trono\", pois pertence ao cluster laranja, que trata de conceitos diferentes (gênero e idade).\n",
    "- \"Menino\" e \"menina\": Estão próximos entre si, refletindo a similaridade semântica entre os termos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687fe4d",
   "metadata": {},
   "source": [
    "##### Gráfico 2: Matriz de Similaridade entre Palavras\n",
    "Este gráfico é um heatmap que mostra a similaridade entre palavras no espaço vetorial. Cada célula representa o produto escalar entre os vetores de duas palavras.\n",
    "\n",
    "O que foi feito:\n",
    "- Matriz de Similaridade: A similaridade foi calculada usando o produto escalar entre os vetores de cada par de palavras.\n",
    "- Visualização com Heatmap: A matriz foi plotada como um mapa de calor para facilitar a interpretação.\n",
    "Interpretação do Heatmap:\n",
    "- Cores mais claras/vermelhas: Indicam maior similaridade entre palavras (produto escalar maior).\n",
    "- Cores mais escuras/azuladas: Indicam menor similaridade entre palavras (produto escalar menor ou negativo).\n",
    "\n",
    "Por exemplo:\n",
    "- A similaridade entre \"rei\" e \"rainha\" é alta (vermelho), indicando que essas palavras compartilham muitos atributos semânticos no corpus.\n",
    "- Palavras como \"rei\" e \"cavaleiro\" também apresentam uma certa similaridade, mas menos intensa do que a entre \"rei\" e \"rainha\".\n",
    "- Palavras como \"trono\" e \"menino\" possuem baixa similaridade, refletindo seu significado distante no corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb33ed8",
   "metadata": {},
   "source": [
    "### Por que utilizar RNNs em Análise de Sentimentos?\n",
    "As Redes Neurais Recorrentes (RNNs) são amplamente utilizadas em tarefas de Processamento de Linguagem Natural (NLP), como análise de sentimentos, devido à sua capacidade de processar dados sequenciais. Aqui estão as razões principais para o uso de RNNs nessa tarefa:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e415d8bf",
   "metadata": {},
   "source": [
    "#### Natureza Sequencial do Texto\n",
    "O texto é uma sequência de palavras, onde o significado de uma palavra muitas vezes depende do contexto em que está inserida.\n",
    "RNNs são projetadas para capturar relações e padrões em dados sequenciais, analisando uma palavra ou caractere considerando as informações anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f039b",
   "metadata": {},
   "source": [
    "#### Memória de Longo Alcance\n",
    "RNNs têm uma memória interna que armazena informações sobre o que já foi processado.\n",
    "Isso permite que a rede compreenda dependências de longo prazo no texto, o que é essencial para capturar nuances de sentimento em frases mais longas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3527a0a5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "##### Capacidade de Modelar Contexto\n",
    "As palavras em um texto podem ter diferentes significados dependendo do contexto.\n",
    "RNNs conseguem capturar essas diferenças, entendendo o sentimento implícito nas relações entre as palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1fc74b",
   "metadata": {},
   "source": [
    "##### Eficiência em Capturar Padrões Temporais\n",
    "- RNNs processam sequências de forma recorrente, atualizando sua memória a cada etapa.\n",
    "- Isso permite que a rede detecte padrões temporais no texto, como negações ou contrastes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c78181",
   "metadata": {},
   "source": [
    "\n",
    "## Redes LSTM\n",
    "\n",
    "As redes LSTM são uma variante das redes neurais recorrentes, projetadas para capturar dependências de longo prazo em dados sequenciais. \n",
    "Nesta seção, treinaremos uma LSTM para prever a próxima palavra em sequências textuais.\n",
    "\n",
    "### Etapas:\n",
    "1. Criar um corpus sintético simples.\n",
    "2. Gerar sequências para treinamento.\n",
    "3. Treinar a LSTM e visualizar a evolução da perda.\n",
    "4. Usar o modelo para gerar texto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Corpus sintético\n",
    "corpus = [\n",
    "    \"rei homem mulher rainha\",\n",
    "    \"rainha trono castelo cavaleiro\",\n",
    "    \"homem menino menina\",\n",
    "    \"castelo cavaleiro rainha\",\n",
    "]\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "\n",
    "# Criar sequências de entrada\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Padding\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\")\n",
    "\n",
    "# Dividir entradas e saídas\n",
    "xs, ys = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "\n",
    "# Modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Treinamento com verbose\n",
    "history = model.fit(xs, ys, epochs=50, verbose=1, batch_size=8)\n",
    "\n",
    "# Visualização da perda\n",
    "plt.plot(history.history[\"loss\"], label=\"Loss\")\n",
    "plt.title(\"Evolução da Perda Durante o Treinamento\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Perda\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Geração de texto\n",
    "seed_text = \"rei homem\"\n",
    "next_words = 3\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=\"pre\")\n",
    "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
    "    output_word = tokenizer.index_word[predicted[0]]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(\"Texto gerado:\", seed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c5962",
   "metadata": {},
   "source": [
    "##### Gráfico da Perda Durante o Treinamento\n",
    "O que é o gráfico de perda?\n",
    "O gráfico mostra como a função de perda do modelo evolui ao longo das épocas de treinamento.\n",
    "A perda é uma métrica que mede o quão bem o modelo está aprendendo a prever a saída correta (neste caso, a próxima palavra na sequência).\n",
    "No início do treinamento, o modelo começa com alta perda porque ainda não \"aprendeu\" padrões no corpus.\n",
    "À medida que o treinamento avança, a perda diminui, indicando que o modelo está melhorando.\n",
    "O que o gráfico mostra?\n",
    "Início com alta perda (2.3):\n",
    "\n",
    "No início, o modelo está apenas começando o treinamento. Ele não conhece o padrão de sequência no corpus.\n",
    "Isso resulta em previsões aleatórias e alta perda.\n",
    "Redução progressiva da perda:\n",
    "\n",
    "A curva desce de forma constante, mostrando que o modelo está aprendendo os padrões no corpus.\n",
    "A redução não é linear, pois em algumas épocas o progresso é menor (indicando que o modelo está refinando o aprendizado).\n",
    "Perda final (~1.7):\n",
    "\n",
    "O modelo estabiliza em torno de 1.7 ao final do treinamento.\n",
    "Isso indica que o modelo aprendeu a gerar sequências com base no corpus, mas ainda pode haver limitações devido ao tamanho do corpus ou à complexidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b99a56",
   "metadata": {},
   "source": [
    "\n",
    "#### Análise dos Resultados\n",
    "A figura apresenta dois elementos principais: o gráfico de perda (loss) durante o treinamento do modelo LSTM e o texto gerado pelo modelo. Vou explicar cada um deles detalhadamente.\n",
    "\n",
    "1. Gráfico da Perda Durante o Treinamento\n",
    "O que é o gráfico de perda?\n",
    "O gráfico mostra como a função de perda do modelo evolui ao longo das épocas de treinamento.\n",
    "A perda é uma métrica que mede o quão bem o modelo está aprendendo a prever a saída correta (neste caso, a próxima palavra na sequência).\n",
    "No início do treinamento, o modelo começa com alta perda porque ainda não \"aprendeu\" padrões no corpus.\n",
    "À medida que o treinamento avança, a perda diminui, indicando que o modelo está melhorando.\n",
    "O que o gráfico mostra?\n",
    "Início com alta perda (2.3):\n",
    "\n",
    "No início, o modelo está apenas começando o treinamento. Ele não conhece o padrão de sequência no corpus.\n",
    "Isso resulta em previsões aleatórias e alta perda.\n",
    "Redução progressiva da perda:\n",
    "\n",
    "A curva desce de forma constante, mostrando que o modelo está aprendendo os padrões no corpus.\n",
    "A redução não é linear, pois em algumas épocas o progresso é menor (indicando que o modelo está refinando o aprendizado).\n",
    "Perda final (~1.7):\n",
    "\n",
    "O modelo estabiliza em torno de 1.7 ao final do treinamento.\n",
    "Isso indica que o modelo aprendeu a gerar sequências com base no corpus, mas ainda pode haver limitações devido ao tamanho do corpus ou à complexidade do modelo.\n",
    "2. Texto Gerado pelo Modelo\n",
    "Texto: \"rei homem rainha rainha rainha\"\n",
    "Como o texto foi gerado?\n",
    "O modelo LSTM foi treinado para prever a próxima palavra em uma sequência.\n",
    "O texto gerado começa com a semente inicial (\"rei homem\") e, a cada passo, o modelo prevê a próxima palavra com base nas palavras anteriores.\n",
    "Por que temos \"rainha rainha rainha\"?\n",
    "Tamanho limitado do corpus:\n",
    "\n",
    "O corpus usado para o treinamento é pequeno, com apenas algumas frases curtas.\n",
    "Isso limita a capacidade do modelo de capturar variações mais amplas no padrão de sequência.\n",
    "Tendência de repetição:\n",
    "\n",
    "\"Rainha\" é uma palavra comum no corpus e ocorre frequentemente em relação a \"rei\" e \"homem\".\n",
    "O modelo pode ter aprendido uma forte associação entre essas palavras, resultando em repetições.\n",
    "Memória limitada da LSTM:\n",
    "\n",
    "Modelos LSTM têm memória limitada, especialmente quando o tamanho da rede é pequeno.\n",
    "Isso pode levar à geração de padrões repetitivos em vez de variações criativas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
